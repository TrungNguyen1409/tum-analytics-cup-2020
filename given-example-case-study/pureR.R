#' ---
#' title: "Business Analytics Case Study"
#' date: 2019-12-12
#' author: TUM - Decision Sciences and Systems (I18)
#' output:
#'   html_document:
#'     df_print: paged
#'     toc: true
#'     toc_float: true
#'     toc_depth: 2
#' ---
#' 
#' # Introduction
#' 
#' ## What is this?
#' 
#' This is an example data analytics case study and was presented in the lecture on 12.12.2019.
#' 
#' In the Analytics Cup, you will receive a few files including
#' 
#' * some data
#' * some background information about the data
#' * clear instructions on what you should do
#' * a test set (without labels).
#' 
#' This file is an _R-markdown_ (`.Rmd`) notebook used to make presentations and reports containing R-Code,
#' output generated by that code and text. In RStudio, you can use the `Knit` function (button in top toolbar) to compile it to an .html or .pdf report.
#' 
#' You should use this file to understand what a typical analytics workflow looks like. During the AC,
#' you may want to use this file as a reference to get started or to look up useful functions and packages. In particular, we demonstrate
#' * How to make some exploratory plots using `ggplot2`.
#' * How the meta-machine learning package `mlr` can be used to apply many different R-machine learning
#' packages with a common interface.
#' 
#' **Note**: _In the analytics cup, you will have to submit the script that generated your submission. This should be a regular R script, not a R-Notebook.
#' 
#' 
#' ## What this is not
#' 
#' * This should _NOT_ be considered a 'perfect' analysis of the given challenge. Some important but menial steps have been ommitted here, e.g.
#'   re-examining and iterating on data preparation after the first model results.
#' * This is _NOT_ a template script for the Analytics Cup in January 2020. You will receive a different challenge and different dataset, so you cannot just replace the data loading with your own and excpect a valid result.
#' * This is _NOT_ a complete overview of useful R tooling, especially we only scratch the surface of
#' what the `mlr` package has to offer. You are advised to read many of the tutorials found at https://mlr.mlr-org.com/. 
#' 
#' ## How is this document structured
#' 
#' First, we will describe the challenge for the case study and the files you would be provided in this case. After that, we will go through the entire workflow once, highlighting useful features of different packages that you may want to use. In particular we
#' 
#' * Load the Data
#' * Explore and clean the data
#' * Load an additional data source and join it to the main data frame
#' * Clean the data and prepare it for use in `mlr`
#' * Create a model setup (task + learner) in `mlr`
#' * Validate the model using cross validation and evaluate the results in `mlr`
#' * Train the final model and make predictions on the test set
#' * Create the submission file that matches the template submission.
#' 
#' # The Challenge
#' 
#' 
#' The dataset in this case study is based on the Global Power Plant Database commissioned by the World Resources Institute: http://datasets.wri.org/dataset/globalpowerplantdatabase
#' 
#' _Citation_:
#' 
#' > Global Energy Observatory, Google, KTH Royal Institute of Technology in Stockholm, Enipedia, World Resources Institute. 2018. Global Power Plant Database. Published on Resource Watch and Google Earth Engine; http://resourcewatch.org/ https://earthengine.google.com/
#' 
#' _GitHub_:
#' >https://github.com/wri/global-power-plant-database
#' 
#' ## Files that would be included in the Analytics Cup
#' * `README.md` a file that clearly outlines your task.
#' * `global_power_plants.csv` - the data
#' * `data_info.txt` - some background information about the data
#' * `submission_template.csv` - the test set and/or a template that specifies the format of a valid submission.
#' 
#' ## Extra files
#' These would not be given in the actual AC but are provided in this case study to give you some more information.
#' 
#' * this file containing a sample solution (`.Rmd` and `.html`).
#' * The Global Power Database PDF report. In it, the authors describe a similar model that was built in 
#' real life (but ultimately not used.)
#' 
#' # Data Analysis
#' 
#' Let's start by loading required packages
#' 
## ---- warning=FALSE------------------------------------------------------
library(tidyverse)
library(lubridate)
library(summarytools)
library(ggmap)
library(mlr) # you might need to install more packages depending on what learner you use in mlr!
options(dplyr.width = Inf) # show all columns when printing to console
theme_set(theme_minimal()) # set ggplot theme for cleaner plotting
set.seed(2019) # in the AC, you'll be required to set a fixed random seed to make your work reproducible

#' 
#' 
#' ## Loading the Data
#'   
## ------------------------------------------------------------------------
read_csv('global_power_plants.csv')

#' 
#' **Note**: If youre reading the html-report, you will only see the first 10k rows, there's actually 29910 rows in the dataset.
#' 
#' There's been problems reading the data (see warnings), let's check why
#' 
## ------------------------------------------------------------------------
read_csv('global_power_plants.csv') %>% problems

#' 
#' Looks like there have been parsing failures for the columns `wepp_id` and `other_fuel3`. Since we care about the latter, we will fix the type in the column specification:
#' 
## ------------------------------------------------------------------------
df <- read_csv('global_power_plants.csv',
               col_types = cols(other_fuel3 = col_character(), wepp_id = col_character()))

#' 
## ------------------------------------------------------------------------
df 

#' 
#' Now the data looks like it has been read in successfully.
#' 
#' 
#' 
#' 
#' ## Exploratory Analysis
#' 
#' The `summarytools` package contains a useful tool to quickly give you a report of your data:
#' 
#' **Note:**  _The output won't be properly displayed if you're viewing the `.html`-report, run the source code yourself to open the summary-report in a browser._
## ------------------------------------------------------------------------
library(summarytools)
df %>% dfSummary %>% view()

#' 
#' There's some columns we definitely won't need in this analysis (see data_info.txt), let's drop them now.
## ------------------------------------------------------------------------
df <- df %>% select(-source, -url, -geolocation_source, -wepp_id,)
df %>% sample_n(10)

#' 
#' Let's fix the data in `commissioning_year` - the column contains decimal numbers, not just full years.
#' 
## ------------------------------------------------------------------------
df <- df %>% 
  mutate(
    commissioning_year = as_date(date_decimal(commissioning_year))
  ) %>% 
  rename(
    id = gppd_idnr,
    commission_date = commissioning_year
  )

#' 
#' 
## ------------------------------------------------------------------------
df

#' 
#' ### Some exploratory plots
#' 
#' #### Power Plants by country
## ------------------------------------------------------------------------
df %>% 
  # only use countries with at least 80 entries
  group_by(country) %>% filter(n() > 80) %>% ungroup() %>%
ggplot( aes(x=country)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  

#' 
#' #### Power Plants by type
#' 
## ------------------------------------------------------------------------
df %>% ggplot(aes(x=primary_fuel, fill=primary_fuel)) +
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#' 
#' #### Total capacity by type
#' 
## ------------------------------------------------------------------------
df %>% ggplot(aes(y=capacity_mw, x= primary_fuel, fill = primary_fuel)) +
  geom_boxplot() +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#' 
#' #### Power mix in top 10 countries by total capacity
#' 
## ------------------------------------------------------------------------
countries_by_capacity <- df %>% group_by(country) %>% summarize(cap = sum(capacity_mw))
top_10_countries <- countries_by_capacity %>% top_n(10, cap) %>% pull(country)
df %>%  filter(country %in% top_10_countries) %>% ggplot(aes(x=country, y=capacity_mw, fill=primary_fuel)) +
  geom_bar(stat='identity') +
  scale_fill_viridis_d()

#' 
#' #### Power mix in top countries, stacked to 100%
#' 
## ------------------------------------------------------------------------
df %>%  filter(country %in% top_10_countries) %>% ggplot(aes(x=country, y=capacity_mw, fill=primary_fuel)) +
  geom_bar(stat='identity', position='fill') +
  scale_y_continuous(labels= scales::percent_format())

#' 
#' #### Exercise:
#' For all nuclear power plants in the U.S., make a line-chart of the development of annual power generation.
#' 
#' Hint: Use `tidyr::pivot_longer` for data preparation to get separate variables 'observation_year' and 'observation_value' per power station, then ggplot with `geom_line` using `aes(..., group=name)`.
#' 
#' 
#' #### Capacity vs generation
#' 
## ---- fig.height=8-------------------------------------------------------
plot <- df %>% 
  # disregard missing values when making the plot scales
  filter(!is.na(generation_gwh_2017)) %>% 
  ggplot(aes(x=capacity_mw, y=generation_gwh_2017, col = year(commission_date), 
             # we can define extra aesthetics that we won't use (see below)
             label=name, label2=country)) +
  geom_point() +
  # plot line indicating full capacity being used (i.e. GWh = MW/1000 * 24*365.25 h)
  stat_function(fun = (function(x) 24*365.25/1000*x), col='grey') +
  facet_wrap(~primary_fuel, scales='free') +
  theme(#legend.position = 'None',
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_viridis_c()
plot

#' 
#' #### Tip: Making a Plot interactive Using `plotly`
#' 
## ------------------------------------------------------------------------
plotly::ggplotly(plot, tooltip = c('x', 'y','label', 'label2', 'col'))

#' 
#' 
#' 
#' In this challenge, we're asked to estimate power generation in 2017 whenever that data is missing,
#' so we can create the test set ourselves.
#' 
## ------------------------------------------------------------------------
train <- df %>% filter(!is.na(generation_gwh_2017))
test <- df %>% anti_join(train)

#' 
#' At this point, we should check whether the test dataset matches the submission template. 
#' (If it doesn't we did something wrong.)
#' 
## ------------------------------------------------------------------------
submission_template <- read_csv('submission_template.csv')
template_ids <- submission_template %>% arrange(id) %>% pull(id)
test_ids <- test %>% arrange(id) %>% pull(id)
all(template_ids == test_ids)

#' 
#' (looks good)
#' 
#' ## Do train and test data look the same?
#' 
#' You should always check this - if there are big differences 
#' * you might not be able to train a model at all (e.g. different columns, different factor-levels)
#' * your model will necessarily be biased if the distribution of features are different.
#' 
#' ### Plotting data on a map
#' 
#' The package `ggmap` offers some nice functionality for plotting geographical data.
#' The easiest way is `ggmap::qmap('world')` , `qmap('Germany')`, `qmap('Tokyo')` etc, to download
#' map data, but you will need a personal Google Maps API key to use qmap. See `?qmap` for details.
#' Note: Make sure not to download too many tiles per month from google or you will incurr Google Cloud charges!
#' 
#' Instead, we will use open source map material from Stamen here, which has unlimited free calls but
#' requires manual configuration of the plotted area.
#' 
## ---- fig.width=10-------------------------------------------------------
left = -180
right = 180
bottom = -60# min(df$latitude)
top = max(df$latitude)
bbox = c(left=left, bottom=bottom, right=right, top=top)

world <- ggmap::get_stamenmap(bbox, zoom = 2, maptype = 'terrain-background')
# see ??get_map for more options
ggmap(world)

#' 
#' 
#' We can now use our downloaded world map and plot things on top of it. 
#' 
#' ### Geographical Location of training and test set instances
#' 
## ---- fig.width=10-------------------------------------------------------
map_data <- df %>% mutate(set = if_else(id %in% train$id, 'train', 'test'))

ggmap(world, ggplot()) +
  geom_point(aes(x=longitude, y=latitude, col=set), size=0.5, data=map_data) +
  ylim(-60, top) +
  theme(legend.position = 'top')

#' 
#' 
#' ### Comparing Summaries of Train and Test set
#' 
#' 
## ------------------------------------------------------------------------
train %>% dfSummary %>% view

#' 
## ------------------------------------------------------------------------
test %>% dfSummary %>% view

#' 
#' Training on this data is problematic
#' * Good quality training data only comes from a handful of countries --> cannot use country as a feature directly
#' * Unclear how other features will translate to other countries
#' * Most test set observations also don't have data for the previous years
#' * Prediction is essentially down to capacity, fuel_type and age of the plant
#' 
#' 
#' ## Enriching Data using External Sources
#' 
#' The following table contains total generation per country, we are explicitly allowed to use it,
#' according to the data challenge instructions.
#' 
#' 
## ------------------------------------------------------------------------
country_data <- read_csv('https://raw.githubusercontent.com/wri/global-power-plant-database/master/resources/generation_by_country_by_fuel_2014.csv')

#' 
## ------------------------------------------------------------------------
country_data

#' 
#' Using this, we will build additional features: 
#' * Plant capacity as share of country's total power generation
#' * Plant capacity as share of coutnry's total power generation of same fuel type
#' 
#' ### Check data quality of country data
#' 
#' Check if country totals match `total` number:
## ------------------------------------------------------------------------
cd_total_given <- country_data %>% filter(fuel=='Total') %>% 
  select(country, total_generation_given = generation_gwh_2014)
cd_total_given

#' 
## ------------------------------------------------------------------------
cd_total_calculated <- country_data %>%
  filter(fuel!='Total') %>% 
  group_by(country) %>% 
  summarize(total_generation_calculated=sum(generation_gwh_2014))
cd_total_calculated

#' 
## ------------------------------------------------------------------------
cd_total_calculated %>% inner_join(cd_total_given) %>% 
  filter(total_generation_calculated != total_generation_given)

#' 
#' 
## ------------------------------------------------------------------------
country_data %>% filter(country == 'Niger')

#' 
#' Let's fix that total mistake in the country data
#' 
## ------------------------------------------------------------------------
country_data[country_data$country=='Niger' & country_data$fuel=='Total', ]$generation_gwh_2014 <- 690

#' 
#' 
#' 
#' ### Joining country data to df
#' 
#' See `?dplyr::join` for more information about join types and basic relational algebra.
#' 
## ------------------------------------------------------------------------
df_joined <- df %>%
  left_join(
    country_data %>% rename(country_gen_by_fuel=generation_gwh_2014),
    by=c("country_long" = "country", "primary_fuel" = "fuel")
    ) %>% 
  left_join(
    country_data %>% filter(fuel == 'Total') %>% transmute(country, country_gen_total = generation_gwh_2014),
    by = c("country_long" = "country")
  )

#' 
## ------------------------------------------------------------------------
df_joined %>% count(is.na(country_gen_by_fuel))

## ------------------------------------------------------------------------
df_joined %>% filter(is.na(country_gen_by_fuel)) %>% count(country_long, sort=T)

#' 
#' 
#' ### check missed entries by type
## ------------------------------------------------------------------------
# missed
df_joined %>% filter(is.na(country_gen_by_fuel)) %>% count(primary_fuel)
# all
df %>% count(primary_fuel)


#' 
#' 
#' At this point we could go back and e.g. change all 'Wave and Tidal' plants to 'Hydro' if you believe that makes sense or take additional steps to augment our training data.
#' Here we will skip this part.
#' 
#' 
#' 
#' ### Actually calculate the additional features features
#' 
## ------------------------------------------------------------------------
df_joined <- df_joined %>% 
  # convert GWh to MW averages
  mutate(
    country_gen_by_fuel = 1000/24/365.25 * country_gen_by_fuel,
    country_gen_total   = 1000/24/365.25 * country_gen_total
  ) %>% 
  mutate(cap_share_of_country_gen_by_fuel = capacity_mw/country_gen_by_fuel,
         cap_share_of_country_gen_total = capacity_mw / country_gen_total)

#' 
#' 
#' ## Some more data cleaning
#' 
#' 
## ------------------------------------------------------------------------
df_joined %>% summary

#' 
#' Our last operation introduced `Inf`, that `mlr` can't impute directly. Let's fix them ourselves
#' (here we arbitrarily choose 1.0, is that a good choice???)
#' 
## ------------------------------------------------------------------------
df_joined <- df_joined %>% mutate(
  cap_share_of_country_gen_by_fuel = if_else(is.infinite(cap_share_of_country_gen_by_fuel), 1.0, cap_share_of_country_gen_by_fuel)
)

#' 
#' 
#' Get rid of all columns that won't be used in the model (except ID)
## ------------------------------------------------------------------------
names(df_joined)
df_joined <- df_joined %>% select(-country, -country_long, -name, -latitude, -longitude,
                     -other_fuel2, -other_fuel3, -owner,
                     -generation_gwh_2013, -generation_gwh_2014, -generation_gwh_2015, -generation_gwh_2016,
                     -country_gen_by_fuel, -country_gen_total)

#' 
## ------------------------------------------------------------------------
train <- df_joined %>% filter(!is.na(generation_gwh_2017))
test <- df_joined %>% anti_join(train) %>% select(-generation_gwh_2017)

## ------------------------------------------------------------------------
dfSummary(train) %>% summarytools::view()

## ------------------------------------------------------------------------
dfSummary(test) %>% summarytools::view()

#' 
#' ## Modeling using MLR
#' 
## ------------------------------------------------------------------------
library(mlr)

#' 
#' ### Data Preparation
#' 
#' Let's write a function that we will apply to both training and test set.
#' 
#' While we could do this manually for each column, mlr provides shortcuts that can save us some typing.
#' There's no clear boundaries what should be done here in the modeling section or what belongs in previous steps - it's up to you.
#' 
#' Note: These are _not_ supposed to be the best imputation methods for this dataset. Other ways may be 
#' better depending on the context of each column.
#' 
## ------------------------------------------------------------------------
prepare_for_mlr <- function(df){
  df_prepared <- df %>% 
    #select(-id) %>% 
    mutate_if(is.Date, decimal_date) %>% 
    mlr::impute(
      classes = list(
        # for each data type, specify a "standard" imputation method to apply
        ## As an example, we'll set NAs in character columns to 'unknown' and numeric columns to their mean
        character = imputeConstant('none'),
        integer = imputeMean(),
        numeric = imputeMean()
        ),
      cols = list(
        # for columns that should NOT use the standard method based on its type, you can overwrite it
        # example: let's impute missing `frequency` values with 0 instead of the mean:
        cap_share_of_country_gen_by_fuel = imputeConstant(0),
        cap_share_of_country_gen_total = imputeConstant(0)
        )
    ) %>% .$data %>% as_tibble() %>% #return a data frame instead of mlr's imputation object
    mutate_if(is.character, as_factor)
}

#' 
#' 
## ------------------------------------------------------------------------
train_ids <- train$id
# remove ids before feeding to mlr
train <- prepare_for_mlr(train)  
test <-  prepare_for_mlr(test) 

#' 
#' 
#' ### Setting up the Task
#' 
#' See `mlr` tutorials for details on tasks, learners, resampling methods, etc.
#' 
## ------------------------------------------------------------------------
task <- makeRegrTask(
  id = 'predict estimated power generation',
  data = train %>% select(-id),
  target = 'generation_gwh_2017'
)

#' 
#' 
#' ### Setting up a Learner
#' 
#' List of possible learners:
#' https://mlr.mlr-org.com/articles/tutorial/integrated_learners.html#regression-59
#' 
#' 
## ------------------------------------------------------------------------
## this requires the package glmnet to be installed (but not loaded)
learner <- makeLearner(
  id = 'my name for my learner',
  cl = 'regr.rpart', #rpart regression tree, replace with model of your choice
  predict.type = 'response',
  fix.factors.prediction = TRUE, # deals with differences factor levels in train and test
  par.vals = list() # set hyperparameters for the learner class given. See learner list above for possible values

)

#' 
#' 
#' Note:
#' 
#' Some learners don't work with factors directly but require you to explicitly one-hot-encode first.
#' You might get an error like
#' ```
#' >  Error in checkLearnerBeforeTrain(task, learner, weights) : 
#' >   Task 'predict future outages' has factor inputs in [columns], but learner [learner] does not support that!
#' ```
#' 
#' If this happens to you, mlr provides a wrapper around the learner that can take care of OHE for you.
#' See this line:
#' `learner <- makeDummyFeaturesWrapper(learner = learner,  method = "1-of-n")`
#' 
#' 
#' ### Setting up a Resampling / Validation Strategy and Tuning the model
#' 
## ------------------------------------------------------------------------
resampling_strategy <- makeResampleDesc("CV", iters = 5)

#' 
#' 
## ------------------------------------------------------------------------
resample_result <- mlr::resample(
  learner = learner,
  task = task,
  resampling = resampling_strategy,
  measures = list(rmse, mae), # specify a list of mlr perormance 'Measures' that you're interested in.
  # list of measures available at https://mlr.mlr-org.com/articles/tutorial/measures.html
  keep.pred = TRUE
)

#' 
## ------------------------------------------------------------------------
resample_result$runtime

#' 
#' After resampling you might want to iterate and tune your learner (see advanced section on mlr website.) When you're done,
#' train your model.
#' 
#' 
#' ## Training the final model
#' 
## ------------------------------------------------------------------------
model <- mlr::train(learner, task)
model

#' 
#' ### Performance Analysis
#' 
#' #### Partial Dependence Plots
#' PD Plots allow you to look at the influence of specific features on the target variable
#' 
#' See the 'advanced' tab on MLR website for more ways to analyze your model data.
#' 
#' 
## ------------------------------------------------------------------------
pd <- generatePartialDependenceData(model, task) #,'primary_fuel')
plotPartialDependence(pd)

#' 
#' 
#' #### Interpreting Results
#' 
#' You can access the actual trained model which is wrapped by mlr using `model$learner.model`
#' 
## ------------------------------------------------------------------------
model$learner.model

#' 
#' 
## ------------------------------------------------------------------------
plot(model$learner.model, compress = TRUE)
text(model$learner.model, use.n = TRUE)

#' 
#' 
#' #### Truth vs Prediction
#' 
#' 
## ------------------------------------------------------------------------
predict(model, task)$data %>%
  ggplot(aes(x=truth, y=response)) + geom_point() +stat_function(fun=identity)

#' 
#' 
#' 
#' ## Predict on the test set
#' 
## ------------------------------------------------------------------------
predictions <- predict(model, newdata=test)$data
test_predicted <- bind_cols(test, predictions)

#' 
#' 
#' ## Create and write submission
#' 
## ------------------------------------------------------------------------
submission <- test_predicted %>% 
  transmute(id, prediction=response) %>% 
  mutate(id = as.character(id)) %>%
  arrange(id)

#' 
## ------------------------------------------------------------------------
submission

#' 
#' Note: in the actual Analytics Cup, the id column might be either character or integer!
#' 
## ------------------------------------------------------------------------
write_csv(submission, 'predictions_MyGroupName_SubmissionNumber.csv')

#' 
#' 
#' 
#' # Some more things we haven't considered here
#' 
#' * dealing with outliers
#' * getting more additional data (e.g. per capita data about the country, elevation/climate, ...)
#' * smart imputation 
#' * model selection and hyperparameter tuning
#' * probabilistic model outputs
#' * feature selection
#' * feature engineering from existing features
#' * feature scaling
#' * choice of relevant performance measures
#' * balancing / over-/undersampling / boosting / ...
#' * ...
