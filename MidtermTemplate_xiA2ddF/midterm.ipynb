{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm: Business Analytics and Machine Learning (IN2028)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"world_map.png\" alt=\"The world map showing the events separated by size and color.\" style=\"width: 1020px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this midterm exam, you will analyze two data sets, *events.csv* and *weather.csv*, to demonstrate your data science skills. Below is a detailed description of these files to guide your work. \n",
    "\n",
    "Your task involves three main components, each with a practical application in the field of data science:\n",
    "\n",
    "1. Cleaning the data sets individually.\n",
    "2. Performing an exploratory analysis.\n",
    "3. Developing a predictive model to classify different types of earthquakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model's performance will be measured by the Balanced Accuracy (BAC) score, where all labels are equally important. The events.csv contains the columns *id* and *mag*. A row containing an entry in the *id* column does not include an entry in the *mag* column and vice versa. You can use every row with an entry in *mag* (and thus no id) to train your model. Remember, it is important to use this data set to evaluate your model. The remaining rows containing an entry in the column *id* depict the data set you should predict. For further instructions on this part, see the Prediction section below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score,roc_auc_score, confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "seed = 2025 # Use this seed for every random operation.\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "submission_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *events.csv* data set contains the following features:\n",
    "\n",
    "* *time*: Time when the event occurred.\n",
    "* *latitude*: Decimal degrees latitude of the event location.\n",
    "* *longitude*: Decimal degrees longitude of the event location.\n",
    "* *depth*: Depth of the event in kilometers.\n",
    "* *mag*: The magnitude of the event.\n",
    "* *magType*: The method or algorithm used to calculate the preferred magnitude for the event. \n",
    "* *dmin*: Horizontal distance from the epicenter to the nearest station in degrees. One degree corresponds to 111.2 kilometers. \n",
    "* *net*: The unique identifier of a data contributor. \n",
    "* *id*: A unique identifier of the event.\n",
    "* *type*: Type of the seismic event.\n",
    "* *horizontalError*: Uncertainty of the reported location of the event. A \"shallow\" value means the error is less than 10km. Otherwise, it is considererd as \"deep\".\n",
    "* *depthError*: Uncertainty of the reported depth of the event in kilometers. \n",
    "* *magError*: The estimated standard error of the reported magnitude of the event.\n",
    "* *is_country*: A binary variable indicating whether an event occured at sea (False) or on land (True).\n",
    "\n",
    "You can assume that there are no outliers in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                   0\n",
      "latitude               0\n",
      "longitude              0\n",
      "depth                  0\n",
      "mag                 3554\n",
      "dmin                1898\n",
      "net                    0\n",
      "id                 10662\n",
      "type                   0\n",
      "horizontalError        0\n",
      "depthError             0\n",
      "magError           13975\n",
      "is_country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "events = pd.read_csv(\"events.csv\")\n",
    "\n",
    "#events.info()\n",
    "print(events.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"time\"] = pd.to_datetime(events['time'])\n",
    "\n",
    "for col in ['magError', 'dmin']:\n",
    "    events[col] = events[col].fillna(events[col].mode()[0])\n",
    "\n",
    "\n",
    "events.rename(columns={\n",
    "    'latitude': 'lat',\n",
    "    'longitude': 'lng',\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                   0\n",
      "lat                    0\n",
      "lng                    0\n",
      "depth                  0\n",
      "mag                 3554\n",
      "dmin                   0\n",
      "net                    0\n",
      "id                 10662\n",
      "type                   0\n",
      "horizontalError        0\n",
      "depthError             0\n",
      "magError               0\n",
      "is_country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "events[\"time\"] = pd.to_datetime(events['time'])\n",
    "\n",
    "for col in ['magError', 'dmin']:\n",
    "    events[col] = events[col].fillna(events[col].mode()[0])\n",
    "\n",
    "print(events.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second data set, which is stored in the *weather.csv* file, contains information about the weather at the events' locations. It contains the following columns:\n",
    "\n",
    "- _time_: The timestamp of the recorded data, indicating the specific date and time of the observation.\n",
    "- _temperature_: The measured air temperature at the given time and location in degrees Celsius.\n",
    "- _humidity_: The relative humidity at the given time and location is expressed as a percentage.\n",
    "- _precipitation_: The amount of precipitation (rainfall and snowfall combined) recorded at the given time and location, typically in millimeters.\n",
    "- _sealevelPressure_: The atmospheric pressure at sea level, recorded at the given time and location, typically in hPa (hectopascals).\n",
    "- _surfacePressure_: The atmospheric pressure at the surface level, recorded at the given time and location, typically in hPa (hectopascals).\n",
    "- _lat_: The latitude coordinate of the location where the event was observed.\n",
    "- _lng_: The longitude coordinate of the location where the event was observed.\n",
    "- _nst_: The minimum number of seismic stations used to determine an event at a specific location.\n",
    "\n",
    "You can assume that each event can uniquely be identified by the date and corresponding location, given by the latitude and longitude values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: In contrast to the event data, the weather dataset provides hourly weather information for each location and date. Before merging this data with the event dataset, aggregate the weather data so that there is only one row per location and date instead of 24 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to focus on predicting whether an earthquake is classified as a strong earthquake (magnitude > 4.4) or a normal earthquake (magnitude <= 4.4).\n",
    "\n",
    "Make sure that your label is as follows:\n",
    "\n",
    "$is\\_high\\_magnitude = \\begin{cases} 1 \\quad \\text{if } mag > 4.4 \\\\ 0 \\quad \\text{else } \\end{cases}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have trained your final model, use it to predict the earthquakes that are associated with an id. Remember, these are those rows in the original *event.csv* that do not have an entry in the *mag* column.\n",
    "\n",
    "The final data frame that contains your prediction needs to follow exactly the following structure:\n",
    "\n",
    "- *id*: The id of an earthquake\n",
    "- *prediction*: The (0 or 1) whether an earthquake has a high magnitude or not.\n",
    "\n",
    "You can use the following *prepare_prediction* function to transform your data set into the correct format. The last cell demonstrates the use of the function and stores your submission into a csv file named submission_x, where x is the number of the submission. Upload this csv file and this jupyter notebook to the acmanager platform: https://analytics-cup.dss.in.tum.de/acm/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction(df, id_column: str = 'id', prediction_column: str = 'is_high_magnitude') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the prediction DataFrame for submission.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame containing the predictions.\n",
    "    id_column (str): The name of the column containing the event IDs.\n",
    "    prediction_column (str): The name of the column containing the predictions.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the required format for submission.\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame contains the required columns\n",
    "    if id_column not in df.columns or prediction_column not in df.columns:\n",
    "        raise ValueError(f\"Columns '{id_column}' and '{prediction_column}' are required in the DataFrame.\")\n",
    "\n",
    "    # Create a copy of the DataFrame with the required columns\n",
    "    submission_df = df[[id_column, prediction_column]].copy()\n",
    "\n",
    "    # Rename the prediction column to 'prediction'\n",
    "    submission_df.rename(columns={prediction_column: 'prediction'}, inplace=True)\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data set\n",
    "submission_df = prepare_prediction(\n",
    "    df = , # Add your data set here\n",
    "    id_column = , # Add the name of the column that contains the ids\n",
    "    prediction_column = , # Add the name of the column that contains your prediction\n",
    ")\n",
    "\n",
    "submission_df.to_csv(f'submission_{submission_counter}', index=False)\n",
    "submission_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
